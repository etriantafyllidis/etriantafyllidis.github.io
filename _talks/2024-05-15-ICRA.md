---
title: "Conference Proceeding Talk - ICRA 2024, Yokohama, Japan"
collection: talks
type: "Conference proceedings talk"
permalink: /talks/2024-05-15-ICRA
venue: "IEEE International Conference on Robotics and Automation (ICRA) 2024"
date: 2024-05-15
location: "Yokohama, Japan"
---

<img src="/images/icra_presentation.png" alt="Description of image" width="300"/>

I presented my paper [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks](https://arxiv.org/abs/2309.16347) at the leading annual Robotics conference IEEE International Conference on Robotics and Automation (ICRA). My presentation was recorded and can be accessed on [YouTube](https://www.youtube.com/watch?v=mRhXfgBllSs). The ICRA conference program can be found [here](https://ras.papercept.net/conferences/conferences/ICRA24/program/ICRA24_ProgramAtAGlanceWeb.html). Many thanks for those who attended and for those that did not, feel free to click on the links above, read our paper and also contact me for any questions, any feedback is most welcome!

<b> Paper Abstract </b>

Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and (iv) maintain robustness against increased levels of uncertainty and horizons.



